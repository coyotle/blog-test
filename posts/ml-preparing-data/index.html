<!doctype html><html lang=ru dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Подготовка данных для нейронной сети | Мини-блог об IT, Linux, Open Source, Tech</title><meta name=keywords content="нейросети,pytorch,"><meta name=description content="На волне шумихи вокруг GPT-3 появилось желание покапаться во внутреннем устройстве нейронных сетей и попробовать написать сеть для классификации текстов по категориям/тэгам. Это первая заметка из серии, речь в ней пойдет о предварительной подготовке данных.
Зачем необходима подготовка данных? Текстовые данные не могут быть использованы напрямую в моделях машинного обучения, так как в нейронах используются простые математические функции которые работают с числовыми данными. Для подготовки текстовых данных используют так называемое кодирование слов - это преобразование текстовых данных в числовые (векторные) представления, которые затем можно использовать для машинного обучения.

Существует много способов кодирования, вот некоторые из них:"><meta name=author content><link rel=canonical href=https://coyotle.ru/posts/ml-preparing-data/><link crossorigin=anonymous href=/assets/css/stylesheet.d6626ca70f3a7dda16dfce9eb29df152c81185d6739a741054093fb92b9e6839.css integrity="sha256-1mJspw86fdoW386esp3xUsgRhdZzmnQQVAk/uSueaDk=" rel="preload stylesheet" as=style><link rel=icon href=https://coyotle.ru/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://coyotle.ru/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://coyotle.ru/favicon-32x32.png><link rel=apple-touch-icon href=https://coyotle.ru/apple-touch-icon.png><link rel=mask-icon href=https://coyotle.ru/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ru href=https://coyotle.ru/posts/ml-preparing-data/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://coyotle.ru/posts/ml-preparing-data/"><meta property="og:site_name" content="Мини-блог об IT, Linux, Open Source, Tech"><meta property="og:title" content="Подготовка данных для нейронной сети"><meta property="og:description" content="На волне шумихи вокруг GPT-3 появилось желание покапаться во внутреннем устройстве нейронных сетей и попробовать написать сеть для классификации текстов по категориям/тэгам. Это первая заметка из серии, речь в ней пойдет о предварительной подготовке данных.
Зачем необходима подготовка данных? Текстовые данные не могут быть использованы напрямую в моделях машинного обучения, так как в нейронах используются простые математические функции которые работают с числовыми данными. Для подготовки текстовых данных используют так называемое кодирование слов - это преобразование текстовых данных в числовые (векторные) представления, которые затем можно использовать для машинного обучения. Существует много способов кодирования, вот некоторые из них:"><meta property="og:locale" content="ru-RU"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-11T23:34:29+03:00"><meta property="article:modified_time" content="2023-02-11T23:34:29+03:00"><meta property="article:tag" content="Нейросети"><meta property="article:tag" content="Pytorch"><meta name=twitter:card content="summary"><meta name=twitter:title content="Подготовка данных для нейронной сети"><meta name=twitter:description content="На волне шумихи вокруг GPT-3 появилось желание покапаться во внутреннем устройстве нейронных сетей и попробовать написать сеть для классификации текстов по категориям/тэгам. Это первая заметка из серии, речь в ней пойдет о предварительной подготовке данных.
Зачем необходима подготовка данных? Текстовые данные не могут быть использованы напрямую в моделях машинного обучения, так как в нейронах используются простые математические функции которые работают с числовыми данными. Для подготовки текстовых данных используют так называемое кодирование слов - это преобразование текстовых данных в числовые (векторные) представления, которые затем можно использовать для машинного обучения.

Существует много способов кодирования, вот некоторые из них:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Посты","item":"https://coyotle.ru/posts/"},{"@type":"ListItem","position":2,"name":"Подготовка данных для нейронной сети","item":"https://coyotle.ru/posts/ml-preparing-data/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Подготовка данных для нейронной сети","name":"Подготовка данных для нейронной сети","description":"На волне шумихи вокруг GPT-3 появилось желание покапаться во внутреннем устройстве нейронных сетей и попробовать написать сеть для классификации текстов по категориям/тэгам. Это первая заметка из серии, речь в ней пойдет о предварительной подготовке данных.\nЗачем необходима подготовка данных? Текстовые данные не могут быть использованы напрямую в моделях машинного обучения, так как в нейронах используются простые математические функции которые работают с числовыми данными. Для подготовки текстовых данных используют так называемое кодирование слов - это преобразование текстовых данных в числовые (векторные) представления, которые затем можно использовать для машинного обучения. Существует много способов кодирования, вот некоторые из них:\n","keywords":["нейросети","pytorch",""],"articleBody":"На волне шумихи вокруг GPT-3 появилось желание покапаться во внутреннем устройстве нейронных сетей и попробовать написать сеть для классификации текстов по категориям/тэгам. Это первая заметка из серии, речь в ней пойдет о предварительной подготовке данных.\nЗачем необходима подготовка данных? Текстовые данные не могут быть использованы напрямую в моделях машинного обучения, так как в нейронах используются простые математические функции которые работают с числовыми данными. Для подготовки текстовых данных используют так называемое кодирование слов - это преобразование текстовых данных в числовые (векторные) представления, которые затем можно использовать для машинного обучения. Существует много способов кодирования, вот некоторые из них:\nOne-hot encoding: представление слов как векторов из нулей и только одним значением 1, соответствующим индексу слова в словаре Count-based encoding: представление слов как векторов, в которых элементы соответствуют количеству вхождений слова в текст. TF-IDF encoding: представление слов как векторов, в которых элементы соответствуют произведению TF (частоты слова в документе) и IDF (обратной документной частоты) Word Embeddings (Word2Vec, GloVe): основывается на взаимодействиях слов в корпусе текстов. Он оптимизирует метрику косинусной близости между векторами слов, чтобы выявлять семантические и синтаксические связи между словами Изначально ковыряюсь с нейронкой я попытался реализовать кодирование One-hot encoding. Хотя этот метод наверное один из самых простых, он требует довольно много памяти т.к. каждое слово кодируется вектором с длинной равной длинне словаря всего текстового корпуса. У меня во время экспериментов модель сжирала все 32ГБ ОЗУ и уходила в своп.\nКак работает one-hon encodding? Например у нас есть тексты:\n['в корзине лежат румяные пирожки', 'в корзине лежат красные яблоки'] Словарь будет состоять из 6 элементов\n['корзина', 'лежать', 'пирожок', 'яблоко', 'румяный', 'красный'] В этом случае слово пирожок (индекс в словаре = 2) будет кодироваться следующим бинарным вектором:\n[0, 0, 1, 0, 0, 0] Если словарь состоит из нескольких тысяч слов - для кодирования каждого слова будет необходим вектор такой же длинны. Т.е. для кодирования текстов этот метод подходит не очень, но его я буду использовать для кодирования категорий/тэгов, т.к. их немного и мне не надо устанавливать взаимоотношения между категориями.\nЗагрузка данных Данные для обучения взял отсюда. Корпус состоит примерно из 850K новостей с сайта lenta.ru. На этапе тестирования, чтобы не парсить все 2ГБ данных, сделаем небольшую выборку из 2000 последних новостей:\ntail -n 2000 data/lenta-ru-news.csv \u003e data/lenta-2000w.csv С помощью pandas загружаем данные и указываем имена столбцов:\nimport pandas as pd df = pd.read_csv('data/lenta-2000w.csv', names=['url', 'title','text','topic','tags','date']) Меня интересуют столбцы text и tags поэтому удаляем лишние столбцы и строки с NaN значениями:\ndf = df.drop(['url','title','topic','date'], axis=1) df = df.dropna() df text\ttags 0\tАктриса Эмма Стоун выйдет замуж, о чем сообщил...\tКино 1\tНа Украине пытаются раскачать ситуацию к возвр...\tУкраина 2\t9 декабря завершится период бесплатного проезд...\tРегионы 3\tЗаместитель генерального директора Российского...\tЛетние виды ...\t...\t... 1969\tИспытание США ранее запрещенной Договором о ли...\tПолитика 1970\tВ ближайшие дни в европейской части России пог...\tОбщество 1971\tВедущие футбольные чемпионаты ушли на зимние к...\tАнглийский футбол 1958 rows × 2 columns Токенизация текста На этом этапе с помощью pymorphy3 и nltk сделаем следующее:\nудалим из текста все символы за исключением А-Яа-я и пробелов разобьем текст на слова с помощь split() удалим из текста стоп-слова и получим их номальную форму Подключаем библиотеки и скачиваем стоп-слова для русского языка:\nimport re import pymorphy3 as pm import nltk from nltk.corpus import stopwords nltk.download('stopwords', download_dir='data/nltk_data') nltk.data.path.append('data/nltk_data') stopwords_ru = stopwords.words('russian') morph = pm.MorphAnalyzer(lang='ru') Функция удаления ненужных символов и преобразования к нормальной форме:\ndef lemmatize(text): text = re.sub(r'[^А-Яа-я ]+', ' ', text) tokens = [] for t in text.split(): tn = morph.normal_forms(t)[0] if (tn not in stopwords_ru): tokens.append(tn) return tokens print(df['text'][3][:200]) print(lemmatize(df['text'][3])[:18]) Заместитель генерального директора Российского антидопингового агентства (РУСАДА) Маргарита Пахноцкая рассказала, что число пойманных на возможных нарушениях антидопинговых правил россиян в 2019 году ['заместитель', 'генеральный', 'директор', 'российский', 'антидопинговый', 'агентство', 'русад', 'маргарита', 'пахноцкий', 'рассказать', 'число', 'поймать', 'возможный', 'нарушение', 'антидопинговый', 'правило', 'россиянин', 'год'] Применяем lemmatize к данным и сохраняем в столбец tokens\ndf['tokens'] = df['text'].apply(lemmatize) df text tags tokens 0 Актриса Эмма Стоун выйдет замуж... Кино [актриса, эмма, стоун, выйти, замуж, сообщить,... 1 На Украине пытаются раскачать ... Украина [на, украина, пытаться, раскачать, ситуация, в... 2 9 декабря завершится период ... Регионы [декабрь, завершиться, период, бесплатный, про... 3 Заместитель генерального дирек... Летние [заместитель, генеральный, директор, российски... ... 1958 rows × 3 columns data = df['tokens'] Создание словаря по датасету from collections import Counter counter = Counter() for tokens in data: for t in tokens: counter[t]+=1 Самые частые слова:\nsorted(counter, key=counter.get, reverse=True)[:10] ['это', 'год', 'который', 'россия', 'декабрь', 'также', 'украина', 'российский', 'свой', 'слово'] Обучение Word2Vec модели для кодирования Для создания word2vec модели будем использовать библиотеку gensim:\nfrom gensim.models import Word2Vec word2vec = Word2Vec(min_count=10, window=2, vector_size=300, negative = 10, alpha=0.03, min_alpha=0.0007, sample=6e-5, sg=1) min_count — игнорировать все слова с частотой встречаемости меньше, чем это значение. windоw — размер контекстного окна, обозначает диапазон контекста. vector_size — размер векторного представления слова (word embedding). negative — сколько неконтекстных слов учитывать в обучении, используя negative sampling. alpha — начальный learning_rate, используемый в алгоритме обратного распространения ошибки (Backpropogation). min_alpha — минимальное значение learning_rate, на которое может опуститься в процессе обучения. sg — если 1, то используется реализация Skip-gram; если 0, то CBOW. Строим словарь:\nword2vec.build_vocab(data) Обучаем модель:\nword2vec.train(data, total_examples=word2vec.corpus_count, epochs=30, report_delay=1) Выведем вектор слова музыка:\nword2vec.wv['музыка'] array([ 1.55311257e-01, -3.71625006e-01, 1.79840580e-01, 2.16249511e-01, -9.86272246e-02, 7.42120668e-02, 2.38929778e-01, 2.27044001e-01, -7.15749860e-02, 1.49555743e-01, -2.27603436e-01, -3.49608958e-01, -1.36643231e-01, -1.11060306e-01, 1.99536532e-01, -2.14455217e-01, ... 8.56719539e-02, -6.46252707e-02, 4.63180207e-02, 4.69253622e-02], dtype=float32) Протестируем модель, выведем близкие слова к слову фильм:\nword2vec.wv.most_similar(positive=[\"фильм\"]) [('хит', 0.7354637384414673), ('композиция', 0.7144984006881714), ('голливудский', 0.7110325694084167), ('кино', 0.7107347249984741), ('сериал', 0.7041939496994019), ('хип', 0.6918789148330688), ('сняться', 0.6890391111373901), ('съёмка', 0.6883667707443237), ('питта', 0.6828123927116394), ('оскар', 0.6809055209159851)] Чем больше коэфициент - тем слова расположены ближе в векторном пространстве.\nИнтересно, что значит питта в этом списке? В контексте кино наверное речь о Брэде Питте, морфологический анализатор не справился с именем. Вообще, выше, где мы собирали словарь для обучения word2vec модели использовалась первая возможная нормальная форма слова morph.normal_forms(t)[0], хотя на самом деле их может быть несколько и без анализа контекста не всегда понятно какая форма правильная. Это задача со звёздочкой ⭐\nВыведем ближайшее к кино слово из списка:\nword2vec.wv.most_similar_to_given(\"кино\", [\"выборы\", \"прокат\", \"актёр\", \"открытие\"]) 'прокат' Ближайшее к слову наука:\nword2vec.wv.most_similar_to_given(\"наука\", [\"выборы\", \"прокат\", \"актёр\", \"открытие\"]) 'открытие' One-hot кодирование тэгов Тут используем тэги как есть, не разбивая на токены и не приводя к нормальной форме, только переведём в нижний регистр.\nСоздаём словарь тэгов:\nfrom torchtext.vocab import Vocab tags_counter = Counter() for tag in df['tags']: tags_counter[tag.lower()]+=1 tags_vocab = Vocab(tags_counter) Используя словарь можно получить номер тэга в этом словаре:\nprint(\"index:\", tags_vocab['культура']) index: 46 Теперь как описывал в начале заметки, кодируем тэг вектором с длинной равной количеству тэгов, все элементы равны нулю, кроме позиции равной номеру тэга в словаре:\ndef tag_to_vect(tag): t_vec = [0] * len(tags_vocab) t_vec[tags_vocab[tag.lower()]] = 1 return t_vec t_vect = tag_to_vect('культура') print(t_vect) print(\"tag vector len:\", len(t_vect)) [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] tag vector len: 74 Словарь будет включать два дополнительных элемента unk и pad.\nПереводим тэги датасета в бинарные векторы:\ndata_y = df['tags'].apply(tag_to_vect) data_y 0 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 1 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ... 1969 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 1970 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 1971 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... Name: tags, Length: 1958, dtype: object После реализации и обучения модели она должна возвращать векторы такой же размерности, где каждый элемент вектора - это вероятность, что данный текст относится к данной категории/тэгу.\nНа этом на сегодня всё.\n","wordCount":"1313","inLanguage":"ru","datePublished":"2023-02-11T23:34:29+03:00","dateModified":"2023-02-11T23:34:29+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://coyotle.ru/posts/ml-preparing-data/"},"publisher":{"@type":"Organization","name":"Мини-блог об IT, Linux, Open Source, Tech","logo":{"@type":"ImageObject","url":"https://coyotle.ru/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://coyotle.ru/ accesskey=h title="Мини-блог об IT, Linux, Open Source, Tech (Alt + H)">Мини-блог об IT, Linux, Open Source, Tech</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://coyotle.ru/tags/ title=тэги><span>тэги</span></a></li><li><a href=https://coyotle.ru/about/ title="обо мне"><span>обо мне</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Подготовка данных для нейронной сети</h1><div class=post-meta><span title='2023-02-11 23:34:29 +0300 +0300'>11 февраля 2023</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Оглавление</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#загрузка-данных>Загрузка данных</a></li><li><a href=#токенизация-текста>Токенизация текста</a></li><li><a href=#создание-словаря-по-датасету>Создание словаря по датасету</a></li><li><a href=#обучение-word2vec-модели-для-кодирования>Обучение Word2Vec модели для кодирования</a></li><li><a href=#one-hot-кодирование-тэгов>One-hot кодирование тэгов</a></li></ul></nav></div></details></div><div class=post-content><p>На волне шумихи вокруг GPT-3 появилось желание покапаться во внутреннем устройстве нейронных сетей и попробовать написать сеть для классификации текстов по категориям/тэгам. Это первая заметка из серии, речь в ней пойдет о предварительной подготовке данных.</p><p>Зачем необходима подготовка данных? Текстовые данные не могут быть использованы напрямую в моделях машинного обучения, так как в нейронах используются простые математические функции которые работают с числовыми данными. Для подготовки текстовых данных используют так называемое кодирование слов - это преобразование текстовых данных в числовые (векторные) представления, которые затем можно использовать для машинного обучения.
<img loading=lazy src=/posts/ml-preparing-data/leo_and_ml.png>
Существует много способов кодирования, вот некоторые из них:</p><ul><li>One-hot encoding: представление слов как векторов из нулей и только одним значением 1, соответствующим индексу слова в словаре</li><li>Count-based encoding: представление слов как векторов, в которых элементы соответствуют количеству вхождений слова в текст.</li><li>TF-IDF encoding: представление слов как векторов, в которых элементы соответствуют произведению TF (частоты слова в документе) и IDF (обратной документной частоты)</li><li>Word Embeddings (Word2Vec, GloVe): основывается на взаимодействиях слов в корпусе текстов. Он оптимизирует метрику косинусной близости между векторами слов, чтобы выявлять семантические и синтаксические связи между словами</li></ul><p>Изначально ковыряюсь с нейронкой я попытался реализовать кодирование One-hot encoding. Хотя этот метод наверное один из самых простых, он требует довольно много памяти т.к. каждое слово кодируется вектором с длинной равной длинне словаря всего текстового корпуса. У меня во время экспериментов модель сжирала все 32ГБ ОЗУ и уходила в своп.</p><p>Как работает one-hon encodding? Например у нас есть тексты:</p><pre tabindex=0><code>[&#39;в корзине лежат румяные пирожки&#39;,
 &#39;в корзине лежат красные яблоки&#39;]
</code></pre><p>Словарь будет состоять из 6 элементов</p><pre tabindex=0><code>[&#39;корзина&#39;, &#39;лежать&#39;, &#39;пирожок&#39;, &#39;яблоко&#39;, &#39;румяный&#39;, &#39;красный&#39;]
</code></pre><p>В этом случае слово <code>пирожок</code> (индекс в словаре = 2) будет кодироваться следующим бинарным вектором:</p><pre tabindex=0><code>[0, 0, 1, 0, 0, 0]
</code></pre><p>Если словарь состоит из нескольких тысяч слов - для кодирования каждого слова будет необходим вектор такой же длинны. Т.е. для кодирования текстов этот метод подходит не очень, но его я буду использовать для кодирования категорий/тэгов, т.к. их немного и мне не надо устанавливать взаимоотношения между категориями.</p><h2 id=загрузка-данных>Загрузка данных<a hidden class=anchor aria-hidden=true href=#загрузка-данных>#</a></h2><p>Данные для обучения взял <a href=https://github.com/yutkin/Lenta.Ru-News-Dataset>отсюда</a>. Корпус состоит примерно из 850K новостей с сайта lenta.ru.
На этапе тестирования, чтобы не парсить все 2ГБ данных, сделаем небольшую выборку из 2000 последних новостей:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>tail -n <span class=m>2000</span> data/lenta-ru-news.csv &gt; data/lenta-2000w.csv
</span></span></code></pre></div><p>С помощью pandas загружаем данные и указываем имена столбцов:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;data/lenta-2000w.csv&#39;</span><span class=p>,</span> <span class=n>names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;url&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;title&#39;</span><span class=p>,</span><span class=s1>&#39;text&#39;</span><span class=p>,</span><span class=s1>&#39;topic&#39;</span><span class=p>,</span><span class=s1>&#39;tags&#39;</span><span class=p>,</span><span class=s1>&#39;date&#39;</span><span class=p>])</span>
</span></span></code></pre></div><p>Меня интересуют столбцы <code>text</code> и <code>tags</code> поэтому удаляем лишние столбцы и строки с NaN значениями:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>drop</span><span class=p>([</span><span class=s1>&#39;url&#39;</span><span class=p>,</span><span class=s1>&#39;title&#39;</span><span class=p>,</span><span class=s1>&#39;topic&#39;</span><span class=p>,</span><span class=s1>&#39;date&#39;</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>df</span>
</span></span></code></pre></div><pre tabindex=0><code>                                                   text	tags
0	Актриса Эмма Стоун выйдет замуж, о чем сообщил...	Кино
1	На Украине пытаются раскачать ситуацию к возвр...	Украина
2	9 декабря завершится период бесплатного проезд...	Регионы
3	Заместитель генерального директора Российского...	Летние виды
...	...	...
1969	Испытание США ранее запрещенной Договором о ли...	Политика
1970	В ближайшие дни в европейской части России пог...	Общество
1971	Ведущие футбольные чемпионаты ушли на зимние к...	Английский футбол
1958 rows × 2 columns
</code></pre><h2 id=токенизация-текста>Токенизация текста<a hidden class=anchor aria-hidden=true href=#токенизация-текста>#</a></h2><p>На этом этапе с помощью pymorphy3 и nltk сделаем следующее:</p><ul><li>удалим из текста все символы за исключением <code>А-Яа-я</code> и пробелов</li><li>разобьем текст на слова с помощь <code>split()</code></li><li>удалим из текста стоп-слова и получим их номальную форму</li></ul><p>Подключаем библиотеки и скачиваем стоп-слова для русского языка:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pymorphy3</span> <span class=k>as</span> <span class=nn>pm</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>nltk</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.corpus</span> <span class=kn>import</span> <span class=n>stopwords</span>
</span></span><span class=line><span class=cl><span class=n>nltk</span><span class=o>.</span><span class=n>download</span><span class=p>(</span><span class=s1>&#39;stopwords&#39;</span><span class=p>,</span> <span class=n>download_dir</span><span class=o>=</span><span class=s1>&#39;data/nltk_data&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>nltk</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s1>&#39;data/nltk_data&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>stopwords_ru</span> <span class=o>=</span> <span class=n>stopwords</span><span class=o>.</span><span class=n>words</span><span class=p>(</span><span class=s1>&#39;russian&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>morph</span> <span class=o>=</span> <span class=n>pm</span><span class=o>.</span><span class=n>MorphAnalyzer</span><span class=p>(</span><span class=n>lang</span><span class=o>=</span><span class=s1>&#39;ru&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p>Функция удаления ненужных символов и преобразования к нормальной форме:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>lemmatize</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;[^А-Яа-я ]+&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>tn</span> <span class=o>=</span> <span class=n>morph</span><span class=o>.</span><span class=n>normal_forms</span><span class=p>(</span><span class=n>t</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>tn</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>stopwords_ru</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tn</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tokens</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>][</span><span class=mi>3</span><span class=p>][:</span><span class=mi>200</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lemmatize</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>][</span><span class=mi>3</span><span class=p>])[:</span><span class=mi>18</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>Заместитель генерального директора Российского антидопингового агентства (РУСАДА) Маргарита Пахноцкая рассказала, что число пойманных на возможных нарушениях антидопинговых правил россиян в 2019 году 
[&#39;заместитель&#39;, &#39;генеральный&#39;, &#39;директор&#39;, &#39;российский&#39;, &#39;антидопинговый&#39;, &#39;агентство&#39;, &#39;русад&#39;, &#39;маргарита&#39;, &#39;пахноцкий&#39;, &#39;рассказать&#39;, &#39;число&#39;, &#39;поймать&#39;, &#39;возможный&#39;, &#39;нарушение&#39;, &#39;антидопинговый&#39;, &#39;правило&#39;, &#39;россиянин&#39;, &#39;год&#39;]
</code></pre><p>Применяем <code>lemmatize</code> к данным и сохраняем в столбец <code>tokens</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;tokens&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>lemmatize</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span>
</span></span></code></pre></div><pre tabindex=0><code>	                        text    tags     tokens
0  Актриса Эмма Стоун выйдет замуж...  Кино     [актриса, эмма, стоун, выйти, замуж, сообщить,...
1  На Украине пытаются раскачать ...   Украина  [на, украина, пытаться, раскачать, ситуация, в...
2  9 декабря завершится период ...     Регионы  [декабрь, завершиться, период, бесплатный, про...
3  Заместитель генерального дирек...   Летние   [заместитель, генеральный, директор, российски...
...
1958 rows × 3 columns
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;tokens&#39;</span><span class=p>]</span>
</span></span></code></pre></div><h2 id=создание-словаря-по-датасету>Создание словаря по датасету<a hidden class=anchor aria-hidden=true href=#создание-словаря-по-датасету>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>Counter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>counter</span> <span class=o>=</span> <span class=n>Counter</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>tokens</span> <span class=ow>in</span> <span class=n>data</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>tokens</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>counter</span><span class=p>[</span><span class=n>t</span><span class=p>]</span><span class=o>+=</span><span class=mi>1</span>
</span></span></code></pre></div><p>Самые частые слова:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>sorted</span><span class=p>(</span><span class=n>counter</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=n>counter</span><span class=o>.</span><span class=n>get</span><span class=p>,</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)[:</span><span class=mi>10</span><span class=p>]</span>
</span></span></code></pre></div><pre tabindex=0><code>[&#39;это&#39;, &#39;год&#39;, &#39;который&#39;, &#39;россия&#39;, &#39;декабрь&#39;, &#39;также&#39;, &#39;украина&#39;, &#39;российский&#39;, &#39;свой&#39;, &#39;слово&#39;]
</code></pre><h2 id=обучение-word2vec-модели-для-кодирования>Обучение Word2Vec модели для кодирования<a hidden class=anchor aria-hidden=true href=#обучение-word2vec-модели-для-кодирования>#</a></h2><p>Для создания word2vec модели будем использовать библиотеку <code>gensim</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>Word2Vec</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>word2vec</span> <span class=o>=</span> <span class=n>Word2Vec</span><span class=p>(</span><span class=n>min_count</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>vector_size</span><span class=o>=</span><span class=mi>300</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                    <span class=n>negative</span> <span class=o>=</span> <span class=mi>10</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.03</span><span class=p>,</span> <span class=n>min_alpha</span><span class=o>=</span><span class=mf>0.0007</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                    <span class=n>sample</span><span class=o>=</span><span class=mf>6e-5</span><span class=p>,</span> <span class=n>sg</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><code>min_count</code> — игнорировать все слова с частотой встречаемости меньше, чем это значение.</li><li><code>windоw</code> — размер контекстного окна, обозначает диапазон контекста.</li><li><code>vector_size</code> — размер векторного представления слова (word embedding).</li><li><code>negative</code> — сколько неконтекстных слов учитывать в обучении, используя negative sampling.</li><li><code>alpha</code> — начальный learning_rate, используемый в алгоритме обратного распространения ошибки (Backpropogation).</li><li><code>min_alpha</code> — минимальное значение learning_rate, на которое может опуститься в процессе обучения.</li><li><code>sg</code> — если 1, то используется реализация Skip-gram; если 0, то CBOW.</li></ul><p>Строим словарь:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>word2vec</span><span class=o>.</span><span class=n>build_vocab</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></div><p>Обучаем модель:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>word2vec</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>total_examples</span><span class=o>=</span><span class=n>word2vec</span><span class=o>.</span><span class=n>corpus_count</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>report_delay</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><p>Выведем вектор слова <code>музыка</code>:</p><pre tabindex=0><code class=language-pyton data-lang=pyton>word2vec.wv[&#39;музыка&#39;]
</code></pre><pre tabindex=0><code>array([ 1.55311257e-01, -3.71625006e-01,  1.79840580e-01,  2.16249511e-01,
       -9.86272246e-02,  7.42120668e-02,  2.38929778e-01,  2.27044001e-01,
       -7.15749860e-02,  1.49555743e-01, -2.27603436e-01, -3.49608958e-01,
       -1.36643231e-01, -1.11060306e-01,  1.99536532e-01, -2.14455217e-01,
...
        8.56719539e-02, -6.46252707e-02,  4.63180207e-02,  4.69253622e-02],
      dtype=float32)
</code></pre><p>Протестируем модель, выведем близкие слова к слову <code>фильм</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>word2vec</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span><span class=n>positive</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;фильм&#34;</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>[(&#39;хит&#39;, 0.7354637384414673),
 (&#39;композиция&#39;, 0.7144984006881714),
 (&#39;голливудский&#39;, 0.7110325694084167),
 (&#39;кино&#39;, 0.7107347249984741),
 (&#39;сериал&#39;, 0.7041939496994019),
 (&#39;хип&#39;, 0.6918789148330688),
 (&#39;сняться&#39;, 0.6890391111373901),
 (&#39;съёмка&#39;, 0.6883667707443237),
 (&#39;питта&#39;, 0.6828123927116394),
 (&#39;оскар&#39;, 0.6809055209159851)]
</code></pre><p>Чем больше коэфициент - тем слова расположены ближе в векторном пространстве.</p><p>Интересно, что значит <code>питта</code> в этом списке? В контексте кино наверное речь о Брэде Питте, морфологический анализатор не справился с именем.
Вообще, выше, где мы собирали словарь для обучения word2vec модели использовалась первая возможная нормальная форма слова <code>morph.normal_forms(t)[0]</code>, хотя на самом деле их может быть несколько и без анализа контекста не всегда понятно какая форма правильная. Это задача со звёздочкой &#x2b50;</p><p>Выведем ближайшее к <code>кино</code> слово из списка:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>word2vec</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>most_similar_to_given</span><span class=p>(</span><span class=s2>&#34;кино&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=s2>&#34;выборы&#34;</span><span class=p>,</span> <span class=s2>&#34;прокат&#34;</span><span class=p>,</span> <span class=s2>&#34;актёр&#34;</span><span class=p>,</span> <span class=s2>&#34;открытие&#34;</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>&#39;прокат&#39;
</code></pre><p>Ближайшее к слову <code>наука</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>word2vec</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>most_similar_to_given</span><span class=p>(</span><span class=s2>&#34;наука&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=s2>&#34;выборы&#34;</span><span class=p>,</span> <span class=s2>&#34;прокат&#34;</span><span class=p>,</span> <span class=s2>&#34;актёр&#34;</span><span class=p>,</span> <span class=s2>&#34;открытие&#34;</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>&#39;открытие&#39;
</code></pre><h2 id=one-hot-кодирование-тэгов>One-hot кодирование тэгов<a hidden class=anchor aria-hidden=true href=#one-hot-кодирование-тэгов>#</a></h2><p>Тут используем тэги как есть, не разбивая на токены и не приводя к нормальной форме, только переведём в нижний регистр.</p><p>Создаём словарь тэгов:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchtext.vocab</span> <span class=kn>import</span> <span class=n>Vocab</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tags_counter</span> <span class=o>=</span> <span class=n>Counter</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>tag</span> <span class=ow>in</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;tags&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>tags_counter</span><span class=p>[</span><span class=n>tag</span><span class=o>.</span><span class=n>lower</span><span class=p>()]</span><span class=o>+=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>tags_vocab</span> <span class=o>=</span> <span class=n>Vocab</span><span class=p>(</span><span class=n>tags_counter</span><span class=p>)</span>
</span></span></code></pre></div><p>Используя словарь можно получить номер тэга в этом словаре:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;index:&#34;</span><span class=p>,</span> <span class=n>tags_vocab</span><span class=p>[</span><span class=s1>&#39;культура&#39;</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>index: 46
</code></pre><p>Теперь как описывал в начале заметки, кодируем тэг вектором с длинной равной количеству тэгов, все элементы равны нулю, кроме позиции равной номеру тэга в словаре:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>tag_to_vect</span><span class=p>(</span><span class=n>tag</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>t_vec</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>tags_vocab</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>t_vec</span><span class=p>[</span><span class=n>tags_vocab</span><span class=p>[</span><span class=n>tag</span><span class=o>.</span><span class=n>lower</span><span class=p>()]]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>t_vec</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>t_vect</span> <span class=o>=</span> <span class=n>tag_to_vect</span><span class=p>(</span><span class=s1>&#39;культура&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>t_vect</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;tag vector len:&#34;</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>t_vect</span><span class=p>))</span>
</span></span></code></pre></div><pre tabindex=0><code>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
tag vector len: 74
</code></pre><blockquote><p>Словарь будет включать два дополнительных элемента unk и pad.</p></blockquote><p>Переводим тэги датасета в бинарные векторы:</p><pre tabindex=0><code>data_y = df[&#39;tags&#39;].apply(tag_to_vect)
data_y
</code></pre><pre tabindex=0><code>0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
1       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
                              ...                        
1969    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
1970    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
1971    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
Name: tags, Length: 1958, dtype: object
</code></pre><p>После реализации и обучения модели она должна возвращать векторы такой же размерности, где каждый элемент вектора - это вероятность, что данный текст относится к данной категории/тэгу.</p><p>На этом на сегодня всё.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://coyotle.ru/tags/%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B8/>Нейросети</a></li><li><a href=https://coyotle.ru/tags/pytorch/>Pytorch</a></li></ul><nav class=paginav><a class=prev href=https://coyotle.ru/posts/ai-and-ethic/><span class=title>« Предыдущая</span><br><span>Этика ИИ. Корпорации врут, а мы катимся в пропасть?</span>
</a><a class=next href=https://coyotle.ru/posts/obsidian-live-sync/><span class=title>Следующая »</span><br><span>Живая синхронизация заметок Obsidian</span></a></nav></footer><div class=comments></div><script>function setComments(e){let t=document.createElement("script");t.src="https://utteranc.es/client.js",t.setAttribute("id","comments-script"),t.setAttribute("repo","coyotle/blog"),t.setAttribute("issue-term","pathname"),t.setAttribute("theme",e),t.setAttribute("label","comment"),t.setAttribute("crossorigin","anonymous"),t.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(t)}document.getElementById("theme-toggle").addEventListener("click",()=>{setComments(document.body.className.includes("dark")?"github-light":"github-dark")});let theme=document.body.className.includes("dark")?"github-dark":"github-light";setComments(theme)</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://coyotle.ru/>Мини-блог об IT, Linux, Open Source, Tech</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="копировать";function s(){t.innerHTML="скопировано!",setTimeout(()=>{t.innerHTML="копировать"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>