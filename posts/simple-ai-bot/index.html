<!doctype html><html lang=ru dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Простой llama-3 телеграм бот | Мини-блог об IT, Linux, Open Source, Tech</title><meta name=keywords content="ai,ии,llm,llama-3,llam,python,telegram"><meta name=description content="В заметке расскажу как на python сделать простого чат бота для телеграм на базе последней версии llm модели llama-3.
Предположим у нас уже установлен python и CUDA (если хотите использовать gpu для ускорения).
Для взаимодействия с моделью на python есть несколько вариантов, чтобы не усложнять будем использовать библиотеку llama.cpp и квантованную модель в формате GGUF. Обратите внимание, нужна Instruct версия.
Подготовка
В телеграм с помощью @BotFather создайте нового бота и получите токен."><meta name=author content><link rel=canonical href=https://coyotle.ru/posts/simple-ai-bot/><link crossorigin=anonymous href=/assets/css/stylesheet.d6626ca70f3a7dda16dfce9eb29df152c81185d6739a741054093fb92b9e6839.css integrity="sha256-1mJspw86fdoW386esp3xUsgRhdZzmnQQVAk/uSueaDk=" rel="preload stylesheet" as=style><link rel=icon href=https://coyotle.ru/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://coyotle.ru/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://coyotle.ru/favicon-32x32.png><link rel=apple-touch-icon href=https://coyotle.ru/apple-touch-icon.png><link rel=mask-icon href=https://coyotle.ru/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ru href=https://coyotle.ru/posts/simple-ai-bot/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://coyotle.ru/posts/simple-ai-bot/"><meta property="og:site_name" content="Мини-блог об IT, Linux, Open Source, Tech"><meta property="og:title" content="Простой llama-3 телеграм бот"><meta property="og:description" content="В заметке расскажу как на python сделать простого чат бота для телеграм на базе последней версии llm модели llama-3.
Предположим у нас уже установлен python и CUDA (если хотите использовать gpu для ускорения). Для взаимодействия с моделью на python есть несколько вариантов, чтобы не усложнять будем использовать библиотеку llama.cpp и квантованную модель в формате GGUF. Обратите внимание, нужна Instruct версия.
Подготовка В телеграм с помощью @BotFather создайте нового бота и получите токен."><meta property="og:locale" content="ru-RU"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-01T15:06:04+03:00"><meta property="article:modified_time" content="2024-05-01T15:06:04+03:00"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Ии"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Llama-3"><meta property="article:tag" content="Llam"><meta property="article:tag" content="Python"><meta property="og:image" content="https://coyotle.ru/cover.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://coyotle.ru/cover.webp"><meta name=twitter:title content="Простой llama-3 телеграм бот"><meta name=twitter:description content="В заметке расскажу как на python сделать простого чат бота для телеграм на базе последней версии llm модели llama-3.
Предположим у нас уже установлен python и CUDA (если хотите использовать gpu для ускорения).
Для взаимодействия с моделью на python есть несколько вариантов, чтобы не усложнять будем использовать библиотеку llama.cpp и квантованную модель в формате GGUF. Обратите внимание, нужна Instruct версия.
Подготовка
В телеграм с помощью @BotFather создайте нового бота и получите токен."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Посты","item":"https://coyotle.ru/posts/"},{"@type":"ListItem","position":2,"name":"Простой llama-3 телеграм бот","item":"https://coyotle.ru/posts/simple-ai-bot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Простой llama-3 телеграм бот","name":"Простой llama-3 телеграм бот","description":"В заметке расскажу как на python сделать простого чат бота для телеграм на базе последней версии llm модели llama-3.\nПредположим у нас уже установлен python и CUDA (если хотите использовать gpu для ускорения). Для взаимодействия с моделью на python есть несколько вариантов, чтобы не усложнять будем использовать библиотеку llama.cpp и квантованную модель в формате GGUF. Обратите внимание, нужна Instruct версия.\nПодготовка В телеграм с помощью @BotFather создайте нового бота и получите токен.\n","keywords":["ai","ии","llm","llama-3","llam","python","telegram"],"articleBody":"В заметке расскажу как на python сделать простого чат бота для телеграм на базе последней версии llm модели llama-3.\nПредположим у нас уже установлен python и CUDA (если хотите использовать gpu для ускорения). Для взаимодействия с моделью на python есть несколько вариантов, чтобы не усложнять будем использовать библиотеку llama.cpp и квантованную модель в формате GGUF. Обратите внимание, нужна Instruct версия.\nПодготовка В телеграм с помощью @BotFather создайте нового бота и получите токен.\nСкачайте с huggingface.co модель, как вариант отсюда https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/tree/main, либо можно найти подходящую по запросу llama-3 8b gguf.\nВ каталоге с проектом создаем виртуальное окружения для python и активируем его:\npython -m venv venv . venv/bin/activate Устанавливаем библиотеку для работы с моделью:\npip install llama-cpp-python Устанавливаем telebot для создания телеграм бота\npip install telebot python-dotenv Работа с моделью Давайте проверим как в принципе взаимодействовать с моделью из python.\nИнициализируем модель:\nfrom llama_cpp import Llama llm = Llama( model_path=\"./models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\", chat_format=\"llama-3\", # n_gpu_layers=-1, # для использования GPU # seed=1337, # установить конкретный seed # n_ctx=8192, # установить размер контекста ) Попробуем получить какой-нибудь вывод от модели:\nmessages = [ { \"role\": \"system\", \"content\": \"Ты полезный ИИ помощник.\" }, { \"role\": \"user\", \"content\": \"Привет! Ты кто?\" }, ] output = llm.create_chat_completion(messages) print(output) Получаем на выходе:\n{ 'id': 'chatcmpl-24b5cdf3-945d-41b2-ad30-d4d24226a4e4', object': 'chat.completion', 'created': 1714558400, 'model': './models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf', 'choices': [ { 'index': 0, 'message': { 'role': 'assistant', 'content': 'Привет! Я - LLaMA, искусственный интеллект, созданный Meta AI. Моя задача - помочь людям в их повседневной жизни, ответить на вопросы, дать советы и просто пообщаться. Я могу генерировать текст, отвечать на вопросы, переводить языки и многое другое! Как я могу помочь вам сегодня?' }, 'logprobs': None, 'finish_reason': 'stop' }], 'usage': { 'prompt_tokens': 33, 'completion_tokens': 88, 'total_tokens': 121 } } Модель работает, можно идти дальше.\nТелеграм бот Теперь сделаем простого бота который будет получать сообщения от пользователя и отправлять ему ответ:\nfrom llama_cpp import Llama import telebot from telebot.types import Message from dotenv import load_dotenv load_dotenv() TG_TOKEN = os.getenv(\"TG_TOKEN\") bot = telebot.TeleBot(TG_TOKEN) @bot.message_handler(commands=[\"start\", \"help\"]) def send_welcome(message: Message): bot.send_message(message.chat.id, \"Я ИИ бот на базе llama-3.\") # Сделаем простую историю общения. Инициализируем ее системным сообщением. messages = [ { \"role\": \"system\", \"content\": \"Ты полезный ИИ помощник.\" } ] # создаем обработчик текстовых сообщений @bot.message_handler(func=lambda message: True) def message_handler(message: Message): chat_id = message.chat.id # добавляем в историю сообщение пользователя messages.append({\"role\": \"user\", \"content\": message.text}) # получаем ответ от модели out = llm.create_chat_completion(messages) # из объекта получаем текст сообщения reply = out[\"choices\"][0][\"message\"][\"content\"] # отправляем текст от ИИ в чат пользователю bot.send_message(chat_id, reply) # сохраняем ответ ИИ в историю messages.append({\"role\": \"assistant\", \"content\": reply}) print(\"bot is ready\") bot.infinity_polling() # запускаем пулинг Создаём .env файл с переменной TG_TOKEN с токеном бота, запускаем скрипт и идем в телеграм к боту проверять: Немного улучшим бота Чат работает, но тут есть как минимум три проблемы:\nОбщая история сообщений для всех пользователей Не учитывается длинна контекста. Для стандартной llama-3 модели размер контекста 8K токенов Немного перепишем бота и сделаем наивное отсечение истории с глубиной в 30 сообщений. Столько последних сообщений бот будет “помнить”.\nimport datetime import os import telebot from dotenv import load_dotenv from llama_cpp import Llama from telebot.types import Message load_dotenv() TG_TOKEN = os.getenv(\"TG_TOKEN\") bot = telebot.TeleBot(TG_TOKEN) @bot.message_handler(commands=[\"start\", \"help\"]) def send_welcome(message: Message): bot.send_message(message.chat.id, \"Я ИИ бот на базе llama-3.\") llm = Llama( model_path=\"./models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\", chat_format=\"llama-3\", verbose=False, ) # словарь для хранения историй сообщений user_message_history = {} @bot.message_handler(content_types=[\"text\"]) def message_handler(message: Message): chat_id = message.chat.id user_id = message.from_user.id # Получаем историю сообщений текущего пользователя user_history = user_message_history.get(user_id, []) user_history.append({\"role\": \"user\", \"content\": message.text}) # Добавим в контест текущую дату и время current_date_time = datetime.datetime.now().strftime(\"%d %B %Y, %H:%M MSK\") messages = [ { \"role\": \"system\", \"content\": f\"Ты полезный ИИ помощник.\\nТекущая дата: {current_date_time}\" } ] for msg in user_history: messages.append(msg) # Симулируем что бот печатает ответ bot.send_chat_action(chat_id, \"typing\") # Получаем ответ от модели out = llm.create_chat_completion(messages) reply = out[\"choices\"][0][\"message\"][\"content\"] # Добавляем ответ бота в историю пользователя user_history.append({\"role\": \"assistant\", \"content\": reply}) # Сохраняем усеченную историю user_message_history[user_id] = user_history[-30:] # Отправляем ответ пользователю bot.send_message(chat_id, reply) print(\"bot is ready\") bot.infinity_polling() # запускаем пулинг Проверим работу контекста и памяти: Бот смог понять из системного сообщения какая сейчас дата и смог вспомнить о чем я его спрашивал.\nОбратите внимание, это proof of concept, бот не для продакшена и в зависимости от ваших ресурсов сможет обработать ограниченное число одновременных (параллельных) запросов. Для чего-то более жизнеспособного можно например организовать очередь сообщений (queue, redis, rabbitqm…) для их последовательной обработки.\n","wordCount":"716","inLanguage":"ru","image":"https://coyotle.ru/cover.webp","datePublished":"2024-05-01T15:06:04+03:00","dateModified":"2024-05-01T15:06:04+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://coyotle.ru/posts/simple-ai-bot/"},"publisher":{"@type":"Organization","name":"Мини-блог об IT, Linux, Open Source, Tech","logo":{"@type":"ImageObject","url":"https://coyotle.ru/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://coyotle.ru/ accesskey=h title="Мини-блог об IT, Linux, Open Source, Tech (Alt + H)">Мини-блог об IT, Linux, Open Source, Tech</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://coyotle.ru/tags/ title=тэги><span>тэги</span></a></li><li><a href=https://coyotle.ru/about/ title="обо мне"><span>обо мне</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Простой llama-3 телеграм бот</h1><div class=post-meta><span title='2024-05-01 15:06:04 +0300 +0300'>1 мая 2024</span></div></header><figure class=entry-cover><img loading=eager srcset='https://coyotle.ru/posts/simple-ai-bot/cover_hu_75d103e4642fb9da.webp 360w,https://coyotle.ru/posts/simple-ai-bot/cover_hu_fe039fe31920d60d.webp 480w,https://coyotle.ru/posts/simple-ai-bot/cover_hu_2113d1266714e7d8.webp 720w,https://coyotle.ru/posts/simple-ai-bot/cover_hu_1a9f78e18828b8c.webp 1080w,https://coyotle.ru/posts/simple-ai-bot/cover.webp 1280w' src=https://coyotle.ru/posts/simple-ai-bot/cover.webp sizes="(min-width: 768px) 720px, 100vw" width=1280 height=841 alt=lama></figure><div class=post-content><p>В заметке расскажу как на python сделать простого чат бота для телеграм на базе последней версии llm модели llama-3.</p><p>Предположим у нас уже установлен python и CUDA (если хотите использовать gpu для ускорения).
Для взаимодействия с моделью на python есть несколько вариантов, чтобы не усложнять будем использовать библиотеку llama.cpp и квантованную модель в формате GGUF. Обратите внимание, нужна Instruct версия.</p><h2 id=подготовка>Подготовка<a hidden class=anchor aria-hidden=true href=#подготовка>#</a></h2><p>В телеграм с помощью <code>@BotFather</code> создайте нового бота и получите токен.</p><p>Скачайте с huggingface.co модель, как вариант отсюда <code>https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/tree/main</code>, либо можно найти подходящую по запросу <code>llama-3 8b gguf</code>.</p><p>В каталоге с проектом создаем виртуальное окружения для python и активируем его:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python -m venv venv
</span></span><span class=line><span class=cl>. venv/bin/activate
</span></span></code></pre></div><p>Устанавливаем библиотеку для работы с моделью:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>pip install llama-cpp-python
</span></span></code></pre></div><p>Устанавливаем telebot для создания телеграм бота</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>pip install telebot python-dotenv
</span></span></code></pre></div><h2 id=работа-с-моделью>Работа с моделью<a hidden class=anchor aria-hidden=true href=#работа-с-моделью>#</a></h2><p>Давайте проверим как в принципе взаимодействовать с моделью из python.</p><p>Инициализируем модель:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_cpp</span> <span class=kn>import</span> <span class=n>Llama</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>Llama</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_path</span><span class=o>=</span><span class=s2>&#34;./models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>chat_format</span><span class=o>=</span><span class=s2>&#34;llama-3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># n_gpu_layers=-1, # для использования GPU</span>
</span></span><span class=line><span class=cl>    <span class=c1># seed=1337, # установить конкретный seed</span>
</span></span><span class=line><span class=cl>    <span class=c1># n_ctx=8192, # установить размер контекста</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Попробуем получить какой-нибудь вывод от модели:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Ты полезный ИИ помощник.&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Привет! Ты кто?&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>create_chat_completion</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><p>Получаем на выходе:</p><pre tabindex=0><code>{
    &#39;id&#39;: &#39;chatcmpl-24b5cdf3-945d-41b2-ad30-d4d24226a4e4&#39;,
    object&#39;: &#39;chat.completion&#39;,
    &#39;created&#39;: 1714558400,
    &#39;model&#39;: &#39;./models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf&#39;,
    &#39;choices&#39;: [
        {
            &#39;index&#39;: 0,
            &#39;message&#39;: {
                &#39;role&#39;: &#39;assistant&#39;,
                &#39;content&#39;: &#39;Привет! Я - LLaMA, искусственный интеллект, созданный Meta AI.
                        Моя задача - помочь людям в их повседневной жизни, ответить на вопросы, дать советы и просто пообщаться.
                        Я могу генерировать текст, отвечать на вопросы, переводить языки и многое другое! Как я могу помочь вам сегодня?&#39;
                },
            &#39;logprobs&#39;: None,
            &#39;finish_reason&#39;: &#39;stop&#39;
        }],
    &#39;usage&#39;: {
        &#39;prompt_tokens&#39;: 33,
        &#39;completion_tokens&#39;: 88,
        &#39;total_tokens&#39;: 121
        }
}
</code></pre><p>Модель работает, можно идти дальше.</p><h2 id=телеграм-бот>Телеграм бот<a hidden class=anchor aria-hidden=true href=#телеграм-бот>#</a></h2><p>Теперь сделаем простого бота который будет получать сообщения от пользователя и отправлять ему ответ:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_cpp</span> <span class=kn>import</span> <span class=n>Llama</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>telebot</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>telebot.types</span> <span class=kn>import</span> <span class=n>Message</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TG_TOKEN</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;TG_TOKEN&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bot</span> <span class=o>=</span> <span class=n>telebot</span><span class=o>.</span><span class=n>TeleBot</span><span class=p>(</span><span class=n>TG_TOKEN</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@bot.message_handler</span><span class=p>(</span><span class=n>commands</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;start&#34;</span><span class=p>,</span> <span class=s2>&#34;help&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>send_welcome</span><span class=p>(</span><span class=n>message</span><span class=p>:</span> <span class=n>Message</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>bot</span><span class=o>.</span><span class=n>send_message</span><span class=p>(</span><span class=n>message</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>&#34;Я ИИ бот на базе llama-3.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Сделаем простую историю общения. Инициализируем ее системным сообщением.</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span> <span class=p>{</span> <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Ты полезный ИИ помощник.&#34;</span> <span class=p>}</span> <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># создаем обработчик текстовых сообщений</span>
</span></span><span class=line><span class=cl><span class=nd>@bot.message_handler</span><span class=p>(</span><span class=n>func</span><span class=o>=</span><span class=k>lambda</span> <span class=n>message</span><span class=p>:</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>message_handler</span><span class=p>(</span><span class=n>message</span><span class=p>:</span> <span class=n>Message</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>chat_id</span> <span class=o>=</span> <span class=n>message</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>id</span>
</span></span><span class=line><span class=cl>    <span class=c1># добавляем в историю сообщение пользователя</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>.</span><span class=n>append</span><span class=p>({</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>message</span><span class=o>.</span><span class=n>text</span><span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=c1># получаем ответ от модели</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>create_chat_completion</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># из объекта получаем текст сообщения</span>
</span></span><span class=line><span class=cl>    <span class=n>reply</span> <span class=o>=</span> <span class=n>out</span><span class=p>[</span><span class=s2>&#34;choices&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;message&#34;</span><span class=p>][</span><span class=s2>&#34;content&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># отправляем текст от ИИ в чат пользователю</span>
</span></span><span class=line><span class=cl>    <span class=n>bot</span><span class=o>.</span><span class=n>send_message</span><span class=p>(</span><span class=n>chat_id</span><span class=p>,</span> <span class=n>reply</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># сохраняем ответ ИИ в историю</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>.</span><span class=n>append</span><span class=p>({</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>reply</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;bot is ready&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bot</span><span class=o>.</span><span class=n>infinity_polling</span><span class=p>()</span> <span class=c1># запускаем пулинг</span>
</span></span></code></pre></div><p>Создаём <code>.env</code> файл с переменной <code>TG_TOKEN</code> с токеном бота, запускаем скрипт и идем в телеграм к боту проверять:
<img loading=lazy src=/posts/simple-ai-bot/hello-message.png></p><h2 id=немного-улучшим-бота>Немного улучшим бота<a hidden class=anchor aria-hidden=true href=#немного-улучшим-бота>#</a></h2><p>Чат работает, но тут есть как минимум три проблемы:</p><ol><li>Общая история сообщений для всех пользователей</li><li>Не учитывается длинна контекста. Для стандартной llama-3 модели размер контекста 8K токенов</li></ol><p>Немного перепишем бота и сделаем наивное отсечение истории с глубиной в 30 сообщений. Столько последних сообщений бот будет &ldquo;помнить&rdquo;.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>datetime</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>telebot</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_cpp</span> <span class=kn>import</span> <span class=n>Llama</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>telebot.types</span> <span class=kn>import</span> <span class=n>Message</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TG_TOKEN</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;TG_TOKEN&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bot</span> <span class=o>=</span> <span class=n>telebot</span><span class=o>.</span><span class=n>TeleBot</span><span class=p>(</span><span class=n>TG_TOKEN</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@bot.message_handler</span><span class=p>(</span><span class=n>commands</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;start&#34;</span><span class=p>,</span> <span class=s2>&#34;help&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>send_welcome</span><span class=p>(</span><span class=n>message</span><span class=p>:</span> <span class=n>Message</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>bot</span><span class=o>.</span><span class=n>send_message</span><span class=p>(</span><span class=n>message</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>&#34;Я ИИ бот на базе llama-3.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>Llama</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_path</span><span class=o>=</span><span class=s2>&#34;./models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>chat_format</span><span class=o>=</span><span class=s2>&#34;llama-3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># словарь для хранения историй сообщений</span>
</span></span><span class=line><span class=cl><span class=n>user_message_history</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@bot.message_handler</span><span class=p>(</span><span class=n>content_types</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;text&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>message_handler</span><span class=p>(</span><span class=n>message</span><span class=p>:</span> <span class=n>Message</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>chat_id</span> <span class=o>=</span> <span class=n>message</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>id</span>
</span></span><span class=line><span class=cl>    <span class=n>user_id</span> <span class=o>=</span> <span class=n>message</span><span class=o>.</span><span class=n>from_user</span><span class=o>.</span><span class=n>id</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Получаем историю сообщений текущего пользователя</span>
</span></span><span class=line><span class=cl>    <span class=n>user_history</span> <span class=o>=</span> <span class=n>user_message_history</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>user_id</span><span class=p>,</span> <span class=p>[])</span>
</span></span><span class=line><span class=cl>    <span class=n>user_history</span><span class=o>.</span><span class=n>append</span><span class=p>({</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>message</span><span class=o>.</span><span class=n>text</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>#  Добавим в контест текущую дату и время</span>
</span></span><span class=line><span class=cl>    <span class=n>current_date_time</span> <span class=o>=</span> <span class=n>datetime</span><span class=o>.</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=o>.</span><span class=n>strftime</span><span class=p>(</span><span class=s2>&#34;</span><span class=si>%d</span><span class=s2> %B %Y, %H:%M MSK&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span> <span class=o>=</span> <span class=p>[</span> <span class=p>{</span> <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;Ты полезный ИИ помощник.</span><span class=se>\n</span><span class=s2>Текущая дата: </span><span class=si>{</span><span class=n>current_date_time</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>}</span> <span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>msg</span> <span class=ow>in</span> <span class=n>user_history</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>msg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Симулируем что бот печатает ответ</span>
</span></span><span class=line><span class=cl>    <span class=n>bot</span><span class=o>.</span><span class=n>send_chat_action</span><span class=p>(</span><span class=n>chat_id</span><span class=p>,</span> <span class=s2>&#34;typing&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Получаем ответ от модели</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>create_chat_completion</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>reply</span> <span class=o>=</span> <span class=n>out</span><span class=p>[</span><span class=s2>&#34;choices&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;message&#34;</span><span class=p>][</span><span class=s2>&#34;content&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Добавляем ответ бота в историю пользователя</span>
</span></span><span class=line><span class=cl>    <span class=n>user_history</span><span class=o>.</span><span class=n>append</span><span class=p>({</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>reply</span><span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=c1># Сохраняем усеченную историю</span>
</span></span><span class=line><span class=cl>    <span class=n>user_message_history</span><span class=p>[</span><span class=n>user_id</span><span class=p>]</span> <span class=o>=</span> <span class=n>user_history</span><span class=p>[</span><span class=o>-</span><span class=mi>30</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Отправляем ответ пользователю</span>
</span></span><span class=line><span class=cl>    <span class=n>bot</span><span class=o>.</span><span class=n>send_message</span><span class=p>(</span><span class=n>chat_id</span><span class=p>,</span> <span class=n>reply</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;bot is ready&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bot</span><span class=o>.</span><span class=n>infinity_polling</span><span class=p>()</span> <span class=c1># запускаем пулинг</span>
</span></span></code></pre></div><p>Проверим работу контекста и памяти:
<img loading=lazy src=/posts/simple-ai-bot/test-memory.png></p><p>Бот смог понять из системного сообщения какая сейчас дата и смог вспомнить о чем я его спрашивал.</p><blockquote><p>Обратите внимание, это proof of concept, бот не для продакшена и в зависимости от ваших ресурсов сможет обработать ограниченное число одновременных (параллельных) запросов.
Для чего-то более жизнеспособного можно например организовать очередь сообщений (queue, redis, rabbitqm&mldr;) для их последовательной обработки.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://coyotle.ru/tags/ai/>Ai</a></li><li><a href=https://coyotle.ru/tags/%D0%B8%D0%B8/>Ии</a></li><li><a href=https://coyotle.ru/tags/llm/>Llm</a></li><li><a href=https://coyotle.ru/tags/llama-3/>Llama-3</a></li><li><a href=https://coyotle.ru/tags/llam/>Llam</a></li><li><a href=https://coyotle.ru/tags/python/>Python</a></li><li><a href=https://coyotle.ru/tags/telegram/>Telegram</a></li></ul><nav class=paginav><a class=prev href=https://coyotle.ru/posts/simple-ai-bot-with-voice/><span class=title>« Предыдущая</span><br><span>Добавляем ИИ боту распознавание голосовых сообщений</span>
</a><a class=next href=https://coyotle.ru/posts/podman-runner/><span class=title>Следующая »</span><br><span>Forgejo Actions в Podman и Quadlet</span></a></nav></footer><div class=comments></div><script>function setComments(e){let t=document.createElement("script");t.src="https://utteranc.es/client.js",t.setAttribute("id","comments-script"),t.setAttribute("repo","coyotle/blog"),t.setAttribute("issue-term","pathname"),t.setAttribute("theme",e),t.setAttribute("label","comment"),t.setAttribute("crossorigin","anonymous"),t.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(t)}document.getElementById("theme-toggle").addEventListener("click",()=>{setComments(document.body.className.includes("dark")?"github-light":"github-dark")});let theme=document.body.className.includes("dark")?"github-dark":"github-light";setComments(theme)</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://coyotle.ru/>Мини-блог об IT, Linux, Open Source, Tech</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="копировать";function s(){t.innerHTML="скопировано!",setTimeout(()=>{t.innerHTML="копировать"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>