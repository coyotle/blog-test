<!doctype html><html lang=ru dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Как обучить языковую модель самостоятельно | Мини-блог об IT, Linux, Open Source, Tech</title><meta name=keywords content="ai,llm,llama,нейросети"><meta name=description content="Мне не давал покоя вопрос, можно ли на моей нищенской RTX3060 12Gb натренировать свою (не)большую языковую модель. И как оказалось - да, это сделать можно используя Low-Rank Adaptation (LoRA). Т.к. VRAM немного, 8B - это самая большая модель из семейства лама которую можно натренировать на этой карте. Что для этого надо?
1. Грабим данные
У меня была идея сделать модель которая будет помогать с трактовкой карт Таро, поэтому идем и грабим корованы сайты с описанием карт и раскладов таро. Для ограбления я написал небольшой python скрипт и с использованием beautifulsoup4 сохранил результат в отдельные JSON файлы."><meta name=author content><link rel=canonical href=https://coyotle.ru/posts/tarot-bot-p1/><link crossorigin=anonymous href=/assets/css/stylesheet.d6626ca70f3a7dda16dfce9eb29df152c81185d6739a741054093fb92b9e6839.css integrity="sha256-1mJspw86fdoW386esp3xUsgRhdZzmnQQVAk/uSueaDk=" rel="preload stylesheet" as=style><link rel=icon href=https://coyotle.ru/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://coyotle.ru/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://coyotle.ru/favicon-32x32.png><link rel=apple-touch-icon href=https://coyotle.ru/apple-touch-icon.png><link rel=mask-icon href=https://coyotle.ru/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ru href=https://coyotle.ru/posts/tarot-bot-p1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://coyotle.ru/posts/tarot-bot-p1/"><meta property="og:site_name" content="Мини-блог об IT, Linux, Open Source, Tech"><meta property="og:title" content="Как обучить языковую модель самостоятельно"><meta property="og:description" content="Мне не давал покоя вопрос, можно ли на моей нищенской RTX3060 12Gb натренировать свою (не)большую языковую модель. И как оказалось - да, это сделать можно используя Low-Rank Adaptation (LoRA). Т.к. VRAM немного, 8B - это самая большая модель из семейства лама которую можно натренировать на этой карте. Что для этого надо?
1. Грабим данные У меня была идея сделать модель которая будет помогать с трактовкой карт Таро, поэтому идем и грабим корованы сайты с описанием карт и раскладов таро. Для ограбления я написал небольшой python скрипт и с использованием beautifulsoup4 сохранил результат в отдельные JSON файлы."><meta property="og:locale" content="ru-RU"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-23T23:37:24+03:00"><meta property="article:modified_time" content="2025-03-23T23:37:24+03:00"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Llama"><meta property="article:tag" content="Нейросети"><meta property="og:image" content="https://coyotle.ru/cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://coyotle.ru/cover.jpg"><meta name=twitter:title content="Как обучить языковую модель самостоятельно"><meta name=twitter:description content="Мне не давал покоя вопрос, можно ли на моей нищенской RTX3060 12Gb натренировать свою (не)большую языковую модель. И как оказалось - да, это сделать можно используя Low-Rank Adaptation (LoRA). Т.к. VRAM немного, 8B - это самая большая модель из семейства лама которую можно натренировать на этой карте. Что для этого надо?
1. Грабим данные
У меня была идея сделать модель которая будет помогать с трактовкой карт Таро, поэтому идем и грабим корованы сайты с описанием карт и раскладов таро. Для ограбления я написал небольшой python скрипт и с использованием beautifulsoup4 сохранил результат в отдельные JSON файлы."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Посты","item":"https://coyotle.ru/posts/"},{"@type":"ListItem","position":2,"name":"Как обучить языковую модель самостоятельно","item":"https://coyotle.ru/posts/tarot-bot-p1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Как обучить языковую модель самостоятельно","name":"Как обучить языковую модель самостоятельно","description":"Мне не давал покоя вопрос, можно ли на моей нищенской RTX3060 12Gb натренировать свою (не)большую языковую модель. И как оказалось - да, это сделать можно используя Low-Rank Adaptation (LoRA). Т.к. VRAM немного, 8B - это самая большая модель из семейства лама которую можно натренировать на этой карте. Что для этого надо?\n1. Грабим данные У меня была идея сделать модель которая будет помогать с трактовкой карт Таро, поэтому идем и грабим корованы сайты с описанием карт и раскладов таро. Для ограбления я написал небольшой python скрипт и с использованием beautifulsoup4 сохранил результат в отдельные JSON файлы.\n","keywords":["ai","llm","llama","нейросети"],"articleBody":"Мне не давал покоя вопрос, можно ли на моей нищенской RTX3060 12Gb натренировать свою (не)большую языковую модель. И как оказалось - да, это сделать можно используя Low-Rank Adaptation (LoRA). Т.к. VRAM немного, 8B - это самая большая модель из семейства лама которую можно натренировать на этой карте. Что для этого надо?\n1. Грабим данные У меня была идея сделать модель которая будет помогать с трактовкой карт Таро, поэтому идем и грабим корованы сайты с описанием карт и раскладов таро. Для ограбления я написал небольшой python скрипт и с использованием beautifulsoup4 сохранил результат в отдельные JSON файлы.\n2. Генерируем тренировочные сэмплы Сграбить сырые данные это даже не пол дела, далее из них надо сделать набор данных для обучения. Тут возможны разные варианты в зависимости от задачи. Если модель должна просто генерировать текст по теме - можно тупо скармливать данные как есть, только почистив их от мусора/html тэгов и т.д. Если модель должна отвечать на впросы по теме - из наших данных надо сделать диалоги между человеком и ассистентом.\nОбрабатывать вручную одному это невозможно, поэтому используем локальную llm которая работает в ollama. Для генерации данных я использовал python и такой промпт:\nТы эксперт по Таро. На основе предоставленного раздела карты сгенерируй 3-5 вопросов и ответов. Формат ответа - JSON массив: { \"questions\" : [ { \"question\": \"текст вопроса\", \"answer\": \"текст ответа\" }, ] } Правила: 1. Вопросы должны относиться только к текущему разделу 2. Ответы 2-4 предложения, содержательные 3. Используй профессиональную терминологию 4. Избегай общих формулировок {CARD_DESCRIPTION} и примерно такой скрипт\nfrom ollama import Client MODEL_NAME=\"phi4\" client = Client(\"http://localhost:11434\") def generate_qa(card_title, section): context = f\"\"\" Карта: {card_title} Раздел: {section['title']} Контент: {section['body'][:2000]} # Обрезка длинных текстов \"\"\" response = client.chat( model=MODEL_NAME, messages=[ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": context}, ], format=\"json\", options={\"temperature\": 0.5}, ) response_data = response.get(\"message\", {}).get(\"content\", \"[]\") qas = json.loads(response_data) qa_pairs = qas.get(\"questions\", \"\") # Валидация структуры if not isinstance(qa_pairs, list): raise ValueError(\"Некорректный формат ответа\") return [ {\"question\": qa.get(\"question\", \"\"), \"answer\": qa.get(\"answer\", \"\")} for qa in qa_pairs ] Попробовал разны модели, для моей задачи лучше всего подошла phi4 от Microsoft. Скармливаем модели наши разделы, получаем ответ, проверяем что это валидный JSON, парсим его и сохраняем в новый набор данных. В результате для каждой карты у меня появились JSON файлы с парами вопрос-ответ:\n{ \"title\": \"Отшельник\", \"qa_pairs\": [ { \"question\": \"Какое общее значение имеет карта Отшельник?\", \"answer\": \"Карта Отшельник символизирует верность себе и необходимость отойти от эмоционально насыщенного образа жизни для внутреннего «исцеления». Она подчеркивает стремление...\" }, ... Плюс я прогнал этот набор еще через одну модель с задачей переформулировать вопрос сохраняя его смысл. В итоге у меня получилось чуть больше 10 тысяч пар вопросов-ответов. Это мало, насколько я понял из прочитанного, для тренировки LoRa неоходимо от 15 тысяч примеров. Поэтому идем и грабим дальше, обрабатываем, сохраняем в нужный формат.\n3. Тренировка (попытка №1) Для тренировки я использовал вот этот jupiter ноутбук от прокекта Unsloth. Он предполагает, что промпт будет в формате Alpaca:\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: {} ### Input: {} ### Response: {} Для этого надо преобразовать данные в массив вида\n[ { \"instruction\": \"Ты ИИ-эксперт по Таро. Ответь на вопрос используя знания о значениях карт и их интерпретациях.\", \"input\": \"Что означает аркан Десятка пентаклей?\", \"output\": \"Десятка пентаклей — это карта, символизирующая материальное изобилие, стабильность ...\" }, ... ] В ноутбуке в разделе train в настройках SFTTrainer комментируем max_steps и добавляем num_train_epochs = N, где N - число эпох.\nМожно начать с одной эпохи чтобы оценить результат, потому что может оказаться, что всё сделано криво. Например у меня loss начал довольно быстро уменьшаться, а потом оказалось, что в секции загрузки данных я криво поменял шаблон и модель обучалась совсем не тому, что я хотел.\nНа RTX 3060 мне понадобилось часа 2 чтобы обучить модель одну эпоху. При тестировании, к моему удивллению, модель даже отвечала по теме! Это успех подумал я, сохранил результат, конвертировал его в GGUF и загрузил в ollama.\nТут оказалось, что модель корректно отвечает только на первый вопрос, а на втором начинает выдавать бесконечное полотно текста. Проблема в том, что формат промпта alpaca предполагает, что модель тренируется только на отдельных “инструкция + вопрос + ответ” и ничего не знает о диалогах. Важно сразу понимать в каком режиме должна работать модель и выбирать соответствующий шаблон данных.\n3. Тренировка (попытка №2) Чтобы модель работала в режиме диалога надо пересобрать данные в цепочки вопрос-ответ (SharedGPT). Я опять же использовал локальную модель чтобы сгенерировать наборы диалогов, получился JSON такого вида:\n[ { \"conversations\": [ { \"from\": \"human\", \"value\": \"Что означает Императрица в раскладе Таро?\" }, { \"from\": \"gpt\", \"value\": \"Императрица символизирует творческую, радостную и созидательную фазу развития...\" }, { \"from\": \"human\", \"value\": \"Как она влияет на личную жизнь?\" }, { \"from\": \"gpt\", \"value\": \"В контексте личной жизни Императрица может предсказывать период стабилизации и гармонизации собственной жизни...\" }, ... ] }, { \"conversations\" : [] В ноутбуке меняем загрузку данных:\nfrom unsloth.chat_templates import get_chat_template tokenizer = get_chat_template( tokenizer, chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style ) def formatting_prompts_func(examples): convos = examples[\"conversations\"] texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos] return { \"text\" : texts, } pass from datasets import load_dataset dataset = load_dataset(\"json\", data_files=\"./tarot_dataset.json\", split = \"train\") dataset = dataset.shuffle(seed=42) # Перемешиваем dataset = dataset.map(formatting_prompts_func, batched = True,) В настройках тренера можно увеличить параметры\nper_device_train_batch_size = 4, gradient_accumulation_steps = 8, warmup_steps = 50, и указываем желаемое число эпох num_train_epochs=3.\nПри таких настройках у меня было Peak reserved memory = 7.699 GB, т.е. использовалось меньше 8ГБ VRAM.\nПосле тренировки сохраняем лору и модель, и можно конвертировать в GGUF:\nfrom unsloth import FastLanguageModel max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally! dtype = ( None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ ) load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False. model, tokenizer = FastLanguageModel.from_pretrained( model_name=\"./models/model_16b\", max_seq_length=max_seq_length, dtype=dtype, load_in_4bit=load_in_4bit, ) # Save to 8bit Q8_0 if True: model.save_pretrained_gguf( \"./models/gguf\", tokenizer, ) # Save to 5bit Q5_K_M if False: model.save_pretrained_gguf( \"./models/gguf\", tokenizer, quantization_method=\"q5_k_m\" ) # Save to 4bit Q4_K_M if False: model.save_pretrained_gguf( \"./models/gguf\", tokenizer, quantization_method=\"q4_k_m\" ) Для конвертации необходимо больше VRAM чем для тренировки, мне приходилось выходить из Gnome и переключаться в tty чтобы освободить память.\nЗагружаем в ollam GGUF-модель используя такой Modelfile\nFROM ./models/gguf_q8/unsloth.Q8_0.gguf TEMPLATE \"\"\"\u003c|begin_of_text|\u003e{{ if .Prompt }}\u003c|start_header_id|\u003euser\u003c|end_header_id|\u003e {{ .Prompt }}\u003c|eot_id|\u003e{{ end }}\u003c|start_header_id|\u003eassistant\u003c|end_header_id|\u003e {{ .Response }}\u003c|eot_id|\u003e\"\"\" PARAMETER stop \"\u003c|start_header_id|\u003e\" PARAMETER stop \"\u003c|end_header_id|\u003e\" PARAMETER stop \"\u003c|eot_id|\u003e\" PARAMETER temperature 0.5 PARAMETER min_p 0.05 PARAMETER top_p 0.85 ollama create tarot:8b_q8 -f ./Modelfile Запускаем:\n$ ollama run tarot:8b_q8 Тестируем базовую модель: И тестируем обученную модель: Видно что базовая модель пишет бессвязную ерунду, а обученная модель текст по теме.\nВ итоге, несколько дней обработки данных + несколько дней экспериментов, и у нас есть своя модель, которая довольно неплохо отвечает на вопросы по теме.\nВыводы Уже сейчас можно натренировать свою большую языковую модель на достаточно бюджетном устройстве и сделать это быстро.\nБольшое значение имеет качество и разнообразие данных.\nДля генерации дополнительных тренировочных данных можно использовать локальные модели, но необходимо валидировать, что они выдают, иногда требуется очиста/постобработка данных после моделей.\nПотестировать конечный результат и поговорить с моделью можно в телеграм https://t.me/ai_tarot_oracle_bot или скачать для локального использования с huggingface.\nТекущая версия обучена на ~20K примерах 5 эпох.\n","wordCount":"1231","inLanguage":"ru","image":"https://coyotle.ru/cover.jpg","datePublished":"2025-03-23T23:37:24+03:00","dateModified":"2025-03-23T23:37:24+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://coyotle.ru/posts/tarot-bot-p1/"},"publisher":{"@type":"Organization","name":"Мини-блог об IT, Linux, Open Source, Tech","logo":{"@type":"ImageObject","url":"https://coyotle.ru/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://coyotle.ru/ accesskey=h title="Мини-блог об IT, Linux, Open Source, Tech (Alt + H)">Мини-блог об IT, Linux, Open Source, Tech</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://coyotle.ru/tags/ title=тэги><span>тэги</span></a></li><li><a href=https://coyotle.ru/about/ title="обо мне"><span>обо мне</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Как обучить языковую модель самостоятельно</h1><div class=post-meta><span title='2025-03-23 23:37:24 +0300 +0300'>23 марта 2025</span></div></header><figure class=entry-cover><img loading=eager srcset='https://coyotle.ru/posts/tarot-bot-p1/cover_hu_f6bd976fb8a27cec.jpg 360w,https://coyotle.ru/posts/tarot-bot-p1/cover_hu_d5e2563ce1ed7010.jpg 480w,https://coyotle.ru/posts/tarot-bot-p1/cover.jpg 600w' src=https://coyotle.ru/posts/tarot-bot-p1/cover.jpg sizes="(min-width: 768px) 720px, 100vw" width=600 height=400 alt></figure><div class=post-content><p>Мне не давал покоя вопрос, можно ли на моей нищенской RTX3060 12Gb натренировать свою (не)большую языковую модель. И как оказалось - да, это сделать можно используя Low-Rank Adaptation (LoRA). Т.к. VRAM немного, 8B - это самая большая модель из семейства лама которую можно натренировать на этой карте. Что для этого надо?</p><h2 id=1-грабим-данные>1. Грабим данные<a hidden class=anchor aria-hidden=true href=#1-грабим-данные>#</a></h2><p>У меня была идея сделать модель которая будет помогать с трактовкой карт Таро, поэтому идем и грабим <del>корованы</del> сайты с описанием карт и раскладов таро. Для ограбления я написал небольшой python скрипт и с использованием beautifulsoup4 сохранил результат в отдельные JSON файлы.</p><h2 id=2-генерируем-тренировочные-сэмплы>2. Генерируем тренировочные сэмплы<a hidden class=anchor aria-hidden=true href=#2-генерируем-тренировочные-сэмплы>#</a></h2><p>Сграбить сырые данные это даже не пол дела, далее из них надо сделать набор данных для обучения. Тут возможны разные варианты в зависимости от задачи. Если модель должна просто генерировать текст по теме - можно тупо скармливать данные как есть, только почистив их от мусора/html тэгов и т.д. Если модель должна отвечать на впросы по теме - из наших данных надо сделать диалоги между человеком и ассистентом.</p><p>Обрабатывать вручную одному это невозможно, поэтому используем локальную llm которая работает в ollama. Для генерации данных я использовал python и такой промпт:</p><pre tabindex=0><code>Ты эксперт по Таро. На основе предоставленного раздела карты сгенерируй 3-5 вопросов и ответов.
Формат ответа - JSON массив:
{
  &#34;questions&#34; : [
    {
      &#34;question&#34;: &#34;текст вопроса&#34;,
      &#34;answer&#34;: &#34;текст ответа&#34;
    },
  ]
}

Правила:
1. Вопросы должны относиться только к текущему разделу
2. Ответы 2-4 предложения, содержательные
3. Используй профессиональную терминологию
4. Избегай общих формулировок

{CARD_DESCRIPTION}
</code></pre><p>и примерно такой скрипт</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ollama</span> <span class=kn>import</span> <span class=n>Client</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>MODEL_NAME</span><span class=o>=</span><span class=s2>&#34;phi4&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>Client</span><span class=p>(</span><span class=s2>&#34;http://localhost:11434&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_qa</span><span class=p>(</span><span class=n>card_title</span><span class=p>,</span> <span class=n>section</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>context</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Карта: </span><span class=si>{</span><span class=n>card_title</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Раздел: </span><span class=si>{</span><span class=n>section</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Контент: </span><span class=si>{</span><span class=n>section</span><span class=p>[</span><span class=s1>&#39;body&#39;</span><span class=p>][:</span><span class=mi>2000</span><span class=p>]</span><span class=si>}</span><span class=s2>  # Обрезка длинных текстов
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span><span class=o>=</span><span class=n>MODEL_NAME</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>SYSTEM_PROMPT</span><span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>context</span><span class=p>},</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=nb>format</span><span class=o>=</span><span class=s2>&#34;json&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>options</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.5</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>response_data</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;message&#34;</span><span class=p>,</span> <span class=p>{})</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;content&#34;</span><span class=p>,</span> <span class=s2>&#34;[]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>qas</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>response_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>qa_pairs</span> <span class=o>=</span> <span class=n>qas</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;questions&#34;</span><span class=p>,</span> <span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Валидация структуры</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>qa_pairs</span><span class=p>,</span> <span class=nb>list</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&#34;Некорректный формат ответа&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;question&#34;</span><span class=p>:</span> <span class=n>qa</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;question&#34;</span><span class=p>,</span> <span class=s2>&#34;&#34;</span><span class=p>),</span> <span class=s2>&#34;answer&#34;</span><span class=p>:</span> <span class=n>qa</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;answer&#34;</span><span class=p>,</span> <span class=s2>&#34;&#34;</span><span class=p>)}</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>qa</span> <span class=ow>in</span> <span class=n>qa_pairs</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span></code></pre></div><p>Попробовал разны модели, для моей задачи лучше всего подошла <code>phi4</code> от Microsoft. Скармливаем модели наши разделы, получаем ответ, проверяем что это валидный JSON, парсим его и сохраняем в новый набор данных. В результате для каждой карты у меня появились JSON файлы с парами вопрос-ответ:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;Отшельник&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;qa_pairs&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;question&#34;</span><span class=p>:</span> <span class=s2>&#34;Какое общее значение имеет карта Отшельник?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;answer&#34;</span><span class=p>:</span> <span class=s2>&#34;Карта Отшельник символизирует верность себе и необходимость отойти от эмоционально насыщенного образа жизни для внутреннего «исцеления». Она подчеркивает стремление...&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=err>...</span>
</span></span></code></pre></div><p>Плюс я прогнал этот набор еще через одну модель с задачей переформулировать вопрос сохраняя его смысл. В итоге у меня получилось чуть больше 10 тысяч пар вопросов-ответов. Это мало, насколько я понял из прочитанного, для тренировки LoRa неоходимо от 15 тысяч примеров. Поэтому идем и грабим дальше, обрабатываем, сохраняем в нужный формат.</p><h2 id=3-тренировка-попытка-1>3. Тренировка (попытка №1)<a hidden class=anchor aria-hidden=true href=#3-тренировка-попытка-1>#</a></h2><p>Для тренировки я использовал вот <a href=https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb>этот</a> jupiter ноутбук от прокекта Unsloth. Он предполагает, что промпт будет в формате Alpaca:</p><pre tabindex=0><code>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}
</code></pre><p>Для этого надо преобразовать данные в массив вида</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;instruction&#34;</span><span class=p>:</span> <span class=s2>&#34;Ты ИИ-эксперт по Таро. Ответь на вопрос используя знания о значениях карт и их интерпретациях.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;Что означает аркан Десятка пентаклей?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;output&#34;</span><span class=p>:</span> <span class=s2>&#34;Десятка пентаклей — это карта, символизирующая материальное изобилие, стабильность ...&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=err>...</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span></code></pre></div><p>В ноутбуке в разделе train в настройках SFTTrainer комментируем <code>max_steps</code> и добавляем <code>num_train_epochs = N</code>, где N - число эпох.</p><p>Можно начать с одной эпохи чтобы оценить результат, потому что может оказаться, что всё сделано криво. Например у меня loss начал довольно быстро уменьшаться, а потом оказалось, что в секции загрузки данных я криво поменял шаблон и модель обучалась совсем не тому, что я хотел.</p><p>На RTX 3060 мне понадобилось часа 2 чтобы обучить модель одну эпоху. При тестировании, к моему удивллению, модель даже отвечала по теме! Это успех подумал я, сохранил результат, конвертировал его в GGUF и загрузил в ollama.</p><p>Тут оказалось, что модель корректно отвечает только на первый вопрос, а на втором начинает выдавать бесконечное полотно текста. Проблема в том, что формат промпта alpaca предполагает, что модель тренируется только на отдельных &ldquo;инструкция + вопрос + ответ&rdquo; и ничего не знает о диалогах. Важно сразу понимать в каком режиме должна работать модель и выбирать соответствующий шаблон данных.</p><h2 id=3-тренировка-попытка-2>3. Тренировка (попытка №2)<a hidden class=anchor aria-hidden=true href=#3-тренировка-попытка-2>#</a></h2><p>Чтобы модель работала в режиме диалога надо пересобрать данные в цепочки вопрос-ответ (SharedGPT). Я опять же использовал локальную модель чтобы сгенерировать наборы диалогов, получился JSON такого вида:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;conversations&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;human&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;Что означает Императрица в раскладе Таро?&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>},</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;Императрица символизирует творческую, радостную и созидательную фазу развития...&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>},</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;human&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;Как она влияет на личную жизнь?&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>},</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;В контексте личной жизни Императрица может предсказывать период стабилизации и гармонизации собственной жизни...&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>},</span>
</span></span><span class=line><span class=cl>      <span class=err>...</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;conversations&#34;</span> <span class=p>:</span> <span class=p>[]</span>
</span></span></code></pre></div><p>В ноутбуке меняем загрузку данных:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>unsloth.chat_templates</span> <span class=kn>import</span> <span class=n>get_chat_template</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>get_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>chat_template</span> <span class=o>=</span> <span class=s2>&#34;llama-3&#34;</span><span class=p>,</span> <span class=c1># Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth</span>
</span></span><span class=line><span class=cl>    <span class=n>mapping</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;role&#34;</span> <span class=p>:</span> <span class=s2>&#34;from&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span> <span class=p>:</span> <span class=s2>&#34;value&#34;</span><span class=p>,</span> <span class=s2>&#34;user&#34;</span> <span class=p>:</span> <span class=s2>&#34;human&#34;</span><span class=p>,</span> <span class=s2>&#34;assistant&#34;</span> <span class=p>:</span> <span class=s2>&#34;gpt&#34;</span><span class=p>},</span> <span class=c1># ShareGPT style</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>formatting_prompts_func</span><span class=p>(</span><span class=n>examples</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>convos</span> <span class=o>=</span> <span class=n>examples</span><span class=p>[</span><span class=s2>&#34;conversations&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>texts</span> <span class=o>=</span> <span class=p>[</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>convo</span><span class=p>,</span> <span class=n>tokenize</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span> <span class=n>add_generation_prompt</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span> <span class=k>for</span> <span class=n>convo</span> <span class=ow>in</span> <span class=n>convos</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span> <span class=s2>&#34;text&#34;</span> <span class=p>:</span> <span class=n>texts</span><span class=p>,</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=k>pass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;json&#34;</span><span class=p>,</span> <span class=n>data_files</span><span class=o>=</span><span class=s2>&#34;./tarot_dataset.json&#34;</span><span class=p>,</span> <span class=n>split</span> <span class=o>=</span> <span class=s2>&#34;train&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>  <span class=c1># Перемешиваем</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>formatting_prompts_func</span><span class=p>,</span> <span class=n>batched</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,)</span>
</span></span></code></pre></div><p>В настройках тренера можно увеличить параметры</p><pre tabindex=0><code>per_device_train_batch_size = 4,
gradient_accumulation_steps = 8,
warmup_steps = 50,
</code></pre><p>и указываем желаемое число эпох <code>num_train_epochs=3</code>.</p><p>При таких настройках у меня было <code>Peak reserved memory = 7.699 GB</code>, т.е. использовалось меньше 8ГБ VRAM.</p><p>После тренировки сохраняем лору и модель, и можно конвертировать в GGUF:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>unsloth</span> <span class=kn>import</span> <span class=n>FastLanguageModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>max_seq_length</span> <span class=o>=</span> <span class=mi>2048</span>  <span class=c1># Choose any! We auto support RoPE Scaling internally!</span>
</span></span><span class=line><span class=cl><span class=n>dtype</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=kc>None</span>  <span class=c1># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>load_in_4bit</span> <span class=o>=</span> <span class=kc>True</span>  <span class=c1># Use 4bit quantization to reduce memory usage. Can be False.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>FastLanguageModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=o>=</span><span class=s2>&#34;./models/model_16b&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_seq_length</span><span class=o>=</span><span class=n>max_seq_length</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>load_in_4bit</span><span class=o>=</span><span class=n>load_in_4bit</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Save to 8bit Q8_0</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>save_pretrained_gguf</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;./models/gguf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Save to 5bit Q5_K_M</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=kc>False</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>save_pretrained_gguf</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;./models/gguf&#34;</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>quantization_method</span><span class=o>=</span><span class=s2>&#34;q5_k_m&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Save to 4bit Q4_K_M</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=kc>False</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>save_pretrained_gguf</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;./models/gguf&#34;</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>quantization_method</span><span class=o>=</span><span class=s2>&#34;q4_k_m&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><p>Для конвертации необходимо больше VRAM чем для тренировки, мне приходилось выходить из Gnome и переключаться в tty чтобы освободить память.</p><p>Загружаем в ollam GGUF-модель используя такой Modelfile</p><pre tabindex=0><code>FROM ./models/gguf_q8/unsloth.Q8_0.gguf

TEMPLATE &#34;&#34;&#34;&lt;|begin_of_text|&gt;{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;

{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

{{ .Response }}&lt;|eot_id|&gt;&#34;&#34;&#34;

PARAMETER stop &#34;&lt;|start_header_id|&gt;&#34;
PARAMETER stop &#34;&lt;|end_header_id|&gt;&#34;
PARAMETER stop &#34;&lt;|eot_id|&gt;&#34;

PARAMETER temperature 0.5
PARAMETER min_p 0.05
PARAMETER top_p 0.85
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>ollama create tarot:8b_q8 -f ./Modelfile
</span></span></code></pre></div><p>Запускаем:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ ollama run tarot:8b_q8
</span></span></code></pre></div><p>Тестируем базовую модель:
<img loading=lazy src=/posts/tarot-bot-p1/test1.png></p><p>И тестируем обученную модель:
<img loading=lazy src=/posts/tarot-bot-p1/test2.png></p><p>Видно что базовая модель пишет бессвязную ерунду, а обученная модель текст по теме.</p><p>В итоге, несколько дней обработки данных + несколько дней экспериментов, и у нас есть своя модель, которая довольно неплохо отвечает на вопросы по теме.</p><h2 id=выводы>Выводы<a hidden class=anchor aria-hidden=true href=#выводы>#</a></h2><p>Уже сейчас можно натренировать свою большую языковую модель на достаточно бюджетном устройстве и сделать это быстро.</p><p>Большое значение имеет качество и разнообразие данных.</p><p>Для генерации дополнительных тренировочных данных можно использовать локальные модели, но необходимо валидировать, что они выдают, иногда требуется очиста/постобработка данных после моделей.</p><p>Потестировать конечный результат и поговорить с моделью можно в телеграм <a href=https://t.me/ai_tarot_oracle_bot>https://t.me/ai_tarot_oracle_bot</a> или скачать для локального использования с <a href=https://huggingface.co/coyotle/ru-tarot>huggingface</a>.</p><p>Текущая версия обучена на ~20K примерах 5 эпох.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://coyotle.ru/tags/ai/>Ai</a></li><li><a href=https://coyotle.ru/tags/llm/>Llm</a></li><li><a href=https://coyotle.ru/tags/llama/>Llama</a></li><li><a href=https://coyotle.ru/tags/%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B8/>Нейросети</a></li></ul><nav class=paginav><a class=prev href=https://coyotle.ru/posts/90s-xl-lora/><span class=title>« Предыдущая</span><br><span>Погружение в 90е (SDXL версия)</span>
</a><a class=next href=https://coyotle.ru/posts/yggdrasil-over-tls/><span class=title>Следующая »</span><br><span>Mesh-сеть Yggdrasil через TLS за Nginx</span></a></nav></footer><div class=comments></div><script>function setComments(e){let t=document.createElement("script");t.src="https://utteranc.es/client.js",t.setAttribute("id","comments-script"),t.setAttribute("repo","coyotle/blog"),t.setAttribute("issue-term","pathname"),t.setAttribute("theme",e),t.setAttribute("label","comment"),t.setAttribute("crossorigin","anonymous"),t.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(t)}document.getElementById("theme-toggle").addEventListener("click",()=>{setComments(document.body.className.includes("dark")?"github-light":"github-dark")});let theme=document.body.className.includes("dark")?"github-dark":"github-light";setComments(theme)</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://coyotle.ru/>Мини-блог об IT, Linux, Open Source, Tech</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="копировать";function s(){t.innerHTML="скопировано!",setTimeout(()=>{t.innerHTML="копировать"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>